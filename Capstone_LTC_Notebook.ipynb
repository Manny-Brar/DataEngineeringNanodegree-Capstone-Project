{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyters3 in /opt/conda/lib/python3.6/site-packages (0.0.43)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Install all tools and import all required libraries^^^'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf, col, to_date, isnan, when, count\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.types     import IntegerType, TimestampType, DoubleType, StructType, StructField\n",
    "from pyspark.sql.types     import *\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "!pip install jupyters3\n",
    "import jupyters3\n",
    "\n",
    "\"\"\"^^^Install all tools and import all required libraries^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^^^Configure AWS access^^^'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\"\"\"^^^Configure AWS access^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session: COMPLETE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Creating Spark session for data processing, if it does not currently exist^^^'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "print('Creating Spark session: COMPLETE')\n",
    "\n",
    "\"\"\"^^^Creating Spark session for data processing, if it does not currently exist^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-emr-resources-795912878947-us-west-2\n",
      "aws-glue-scripts-795912878947-us-west-2\n",
      "aws-glue-temporary-795912878947-us-west-2\n",
      "aws-logs-795912878947-us-west-2\n",
      "capston-bucket2\n",
      "dend-capstone-bucket2\n",
      "dend-capstone-crypto-bucket\n",
      "dend-capstone-output\n",
      "dend-s3-buckey\n",
      "im-cap\n",
      "p4-buckey\n",
      "p5-bucket\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^ Utilize boto3 library to verify connection by printing out S3 buckets^^^'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "\n",
    "\"\"\"^^^ Utilize boto3 library to verify connection by printing out S3 buckets^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- open: string (nullable = true)\n",
      " |-- close: string (nullable = true)\n",
      " |-- high: string (nullable = true)\n",
      " |-- low: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- results: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- close: double (nullable = true)\n",
      " |    |    |-- high: double (nullable = true)\n",
      " |    |    |-- low: double (nullable = true)\n",
      " |    |    |-- open: double (nullable = true)\n",
      " |    |    |-- time: long (nullable = true)\n",
      " |    |    |-- volume: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Read in S3 data using Spark and print schema^^^'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTC_csv=spark.read.csv('s3a://dend-capstone-crypto-bucket/ltcusd.csv', header=True)\n",
    "ETH_json=spark.read.json('s3a://capston-bucket2/ethusd.json')\n",
    "LTC_csv.printSchema()\n",
    "ETH_json.printSchema()\n",
    "\"\"\"^^^Read in S3 data using Spark and print schema^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTC_csv dataframe shape: (1663435, 6)\n",
      "+----+----+-----+----+---+------+\n",
      "|time|open|close|high|low|volume|\n",
      "+----+----+-----+----+---+------+\n",
      "|   0|   0|    0|   0|  0|     0|\n",
      "+----+----+-----+----+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Data Quality check #1--Check shape of dataframe and if any columns contain NaN values^^^'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'LTC_csv dataframe shape:',(LTC_csv.count(), len(LTC_csv.columns)))\n",
    "LTC_csv.select([count(when(isnan(c), c)).alias(c) for c in LTC_csv.columns]).show()\n",
    "\"\"\"^^^Data Quality check #1--Check shape of dataframe and if any columns contain NaN values^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_ltc(spark, input_data, output_data):\n",
    "    print('Processing xrp data from S3 bucket...')\n",
    "    \"\"\"\n",
    "    This function will:  Extract and process data to \n",
    "    create (df)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ltc_data = input_data\n",
    "    df = LTC_csv\n",
    "    print('Reading ltc data from S3 bucket: COMPLETE')\n",
    "    \n",
    "    \"\"\"^^^Read in data from S3 bucket and assign it as df^^^\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"^^^Convert Spark dataframe to pandas dataframe & set index as 'time'^^^\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_pred=30 #References how far out the prediction model will predict\n",
    "#coin_to_predict= \"ltcusd\"    \n",
    "\n",
    "main_df=pd.DataFrame()\n",
    "df=LTC_csv.toPandas()\n",
    "df.set_index(\"time\", inplace=True)\n",
    "    \n",
    "if len(main_df)== 0:\n",
    "    main_df=df\n",
    "else:\n",
    "    main_df = main_df.join(df)\n",
    "        \n",
    "\"\"\"^^^Convert Spark dataframe to pandas dataframe & set index as 'time'^^^\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"^^^Create new df as ltc_df with only 'close' and 'LTC_future' columns extracted from main_df^^^\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.fillna(method='bfill', inplace=True)\n",
    "main_df.dropna(inplace=True)\n",
    "\"\"\"^^^Back fill close data where values were NaN^^^\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "main_df['LTC_future']= main_df[\"close\"].shift(-future_pred)\n",
    "main_df.dropna(inplace=True)\n",
    "ltc_df=main_df[['close', 'LTC_future']]\n",
    "ltc_df.head()\n",
    "\"\"\"^^^Create new df as ltc_df with only 'close' and 'LTC_future' columns extracted from main_df^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [['3.1491']\n",
      " ['3.1491']\n",
      " ['3.1491']\n",
      " ..., \n",
      " ['57.695']\n",
      " ['57.706']\n",
      " ['57.716']]\n",
      "y: ['3.05' '3.05' '3.05' ..., '58.03100000000001' '58.095' '58.066']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['57.737'],\n",
       "       ['57.7'],\n",
       "       ['57.701'],\n",
       "       ['57.745'],\n",
       "       ['57.746'],\n",
       "       ['57.799'],\n",
       "       ['57.775'],\n",
       "       ['57.784'],\n",
       "       ['57.883'],\n",
       "       ['57.878'],\n",
       "       ['57.841'],\n",
       "       ['57.867'],\n",
       "       ['57.95'],\n",
       "       ['57.95'],\n",
       "       ['57.93'],\n",
       "       ['57.84'],\n",
       "       ['57.8'],\n",
       "       ['57.8'],\n",
       "       ['57.808'],\n",
       "       ['57.86'],\n",
       "       ['57.8'],\n",
       "       ['57.8'],\n",
       "       ['57.808'],\n",
       "       ['57.782'],\n",
       "       ['57.839'],\n",
       "       ['57.833'],\n",
       "       ['57.73'],\n",
       "       ['57.695'],\n",
       "       ['57.706'],\n",
       "       ['57.716']], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"___Begin building model for LTC future prediction___\"\"\"\n",
    "\n",
    "X= np.array(ltc_df.drop(['LTC_future'], 1))[:-future_pred]\n",
    "print('x:', X)\n",
    "Y=np.array(ltc_df['LTC_future'])[:-future_pred]\n",
    "print('y:',Y)\n",
    "\n",
    "#Split data 75% training\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\"\"\"^^^Split training and test data with 75/25 split^^^\"\"\"\n",
    "\n",
    "# Decision tree regressor model\n",
    "tree= DecisionTreeRegressor().fit(x_train, y_train)\n",
    "#Linear Regression model\n",
    "lr= LinearRegression().fit(x_train, y_train)\n",
    "\"\"\"^^^Create Decision Tree and Linear Regresion Models^^^\"\"\"\n",
    "\n",
    "#Get last rows of future\n",
    "x_future= ltc_df.drop(['LTC_future'], 1)[:-future_pred]\n",
    "x_future= x_future.tail(future_pred)\n",
    "x_future= np.array(x_future)\n",
    "x_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decission Tree: [ 57.85740909  57.80831304  57.82542857  57.69136364  57.77226316\n",
      "  57.83123077  57.8165      57.74213333  57.94093333  57.87541379\n",
      "  58.08811765  57.97028571  57.97265079  57.97265079  57.86448718\n",
      "  57.88075676  57.86157265  57.86157265  57.87467742  57.90087234\n",
      "  57.86157265  57.86157265  57.87467742  57.81052632  57.784       57.67472\n",
      "  57.70281667  57.6299      57.6514      57.56809091]\n",
      "\n",
      "Linear Regression: [ 57.74037452  57.70338218  57.70438197  57.74837287  57.74937266\n",
      "  57.8023617   57.77836666  57.7873648   57.88634432  57.88134536\n",
      "  57.84435301  57.87034763  57.95333046  57.95333046  57.9333346\n",
      "  57.84335322  57.80336149  57.80336149  57.81135984  57.86334908\n",
      "  57.80336149  57.80336149  57.81135984  57.78536522  57.84235342\n",
      "  57.83635467  57.73337597  57.69838321  57.70938094  57.71937887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"^^^Convert the 'time' column from epoch seconds to pandas datetime^^^\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show model tree prediction\n",
    "tree_pred= tree.predict(x_future)\n",
    "print(f'Decission Tree: {tree_pred}')\n",
    "print()\n",
    "#Show Linear model pred\n",
    "lr_pred= lr.predict(x_future)\n",
    "print(f'Linear Regression: {lr_pred}')\n",
    "\n",
    "\n",
    "predictions= tree_pred\n",
    "valid_tree= ltc_df[X.shape[0]:]\n",
    "valid_tree['Predictions']= predictions\n",
    "valid_tree= valid_tree.apply(pd.to_numeric)\n",
    "valid_tree['Error_%']=(valid_tree['LTC_future']-valid_tree['Predictions'])/valid_tree['LTC_future']*(100)\n",
    "valid_tree['LTC_future'].dtype\n",
    "\n",
    "\"\"\"^^^Create new column 'Error_%' to view the percentage of error between the predictions and actual future value.\n",
    "[((future value) - (prediction value) / (future value))* 100] ^^^\"\"\"\n",
    "\n",
    "valid_tree.reset_index(inplace=True) \n",
    "valid_tree.head()\n",
    "\"\"\"^^^Reset index for valid_tree dataframe^^^\"\"\"\n",
    "\n",
    "predictions= lr_pred\n",
    "valid_lr= ltc_df[X.shape[0]:]\n",
    "valid_lr['Predictions']= predictions\n",
    "valid_lr= valid_lr.apply(pd.to_numeric)\n",
    "valid_lr['Error_%']=(valid_lr['LTC_future']-valid_lr['Predictions'])/valid_lr['LTC_future']*(100)\n",
    "valid_lr.reset_index(inplace=True)\n",
    "valid_lr.head()\n",
    "\"\"\"^^^Create new column 'Error_%' to view the percentage of error between the predictions and actual future value.\n",
    "[((future value) - (prediction value) / (future value))* 100] ^^^\"\"\"\n",
    "\"\"\"^^^Reset index for valid_tree dataframe^^^\"\"\"\n",
    "\n",
    "\n",
    "valid_tree['time'] = pd.to_datetime(valid_tree['time'],unit='ms')\n",
    "valid_lr['time'] = pd.to_datetime(valid_lr['time'],unit='ms')\n",
    "\"\"\"^^^Convert the 'time' column from epoch seconds to pandas datetime^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- LTC_future: float (nullable = true)\n",
      " |-- Predictions: float (nullable = true)\n",
      " |-- Error_%: float (nullable = true)\n",
      "\n",
      "+-------------------+------+----------+-----------+-----------+\n",
      "|               time| close|LTC_future|Predictions|    Error_%|\n",
      "+-------------------+------+----------+-----------+-----------+\n",
      "|2020-08-27 07:26:00|57.869|     58.08|   57.85741|  0.3832488|\n",
      "|2020-08-27 07:29:00|58.043|    58.083|   57.80831| 0.47292143|\n",
      "|2020-08-27 07:36:00|58.135|    58.052|  57.825428| 0.39029047|\n",
      "|2020-08-27 07:37:00|58.205|    58.124|  57.691364| 0.74433345|\n",
      "|2020-08-27 07:38:00| 58.41|     58.12|  57.772263|  0.5983084|\n",
      "|2020-08-27 07:39:00|58.449|    58.101|   57.83123| 0.46431082|\n",
      "|2020-08-27 07:40:00|58.222|    58.082|    57.8165| 0.45711237|\n",
      "|2020-08-27 07:42:00| 58.38|    58.088|  57.742134| 0.59541845|\n",
      "|2020-08-27 07:43:00| 58.37|    58.088|  57.940933| 0.25317907|\n",
      "|2020-08-27 07:46:00|58.286|    57.971|  57.875412| 0.16488625|\n",
      "|2020-08-27 07:47:00|58.329|    58.044|   58.08812|-0.07600725|\n",
      "|2020-08-27 07:51:00|58.363|    58.182|  57.970287| 0.36388278|\n",
      "|2020-08-27 07:59:00|58.326|    58.344|  57.972652| 0.63648224|\n",
      "|2020-08-27 08:00:00|58.237|    58.355|  57.972652|  0.6552124|\n",
      "|2020-08-27 08:04:00| 58.14|    58.321|  57.864487|  0.7827589|\n",
      "|2020-08-27 08:06:00|58.146|    58.327|  57.880756|  0.7650715|\n",
      "|2020-08-27 08:07:00|58.155|    58.318|  57.861572|  0.7826526|\n",
      "|2020-08-27 08:08:00|58.143|    58.282|  57.861572|  0.7213674|\n",
      "|2020-08-27 08:20:00|58.117|    58.307|  57.874676| 0.74145913|\n",
      "|2020-08-27 08:22:00| 58.15|    58.242|   57.90087|  0.5857073|\n",
      "+-------------------+------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- LTC_future: float (nullable = true)\n",
      " |-- Predictions: float (nullable = true)\n",
      " |-- Error_%: float (nullable = true)\n",
      "\n",
      "+-------------------+------+----------+-----------+----------+\n",
      "|               time| close|LTC_future|Predictions|   Error_%|\n",
      "+-------------------+------+----------+-----------+----------+\n",
      "|2020-08-27 07:26:00|57.869|     58.08|  57.740376| 0.5847546|\n",
      "|2020-08-27 07:29:00|58.043|    58.083|   57.70338|0.65357816|\n",
      "|2020-08-27 07:36:00|58.135|    58.052|  57.704384| 0.5988046|\n",
      "|2020-08-27 07:37:00|58.205|    58.124|   57.74837| 0.6462513|\n",
      "|2020-08-27 07:38:00| 58.41|     58.12|  57.749374| 0.6376933|\n",
      "|2020-08-27 07:39:00|58.449|    58.101|   57.80236|0.51399857|\n",
      "|2020-08-27 07:40:00|58.222|    58.082|  57.778366|0.52276665|\n",
      "|2020-08-27 07:42:00| 58.38|    58.088|  57.787365| 0.5175513|\n",
      "|2020-08-27 07:43:00| 58.37|    58.088|  57.886345|0.34715548|\n",
      "|2020-08-27 07:46:00|58.286|    57.971|  57.881344| 0.1546543|\n",
      "|2020-08-27 07:47:00|58.329|    58.044|  57.844353|0.34395802|\n",
      "|2020-08-27 07:51:00|58.363|    58.182|  57.870346|0.53565085|\n",
      "|2020-08-27 07:59:00|58.326|    58.344|   57.95333| 0.6695968|\n",
      "|2020-08-27 08:00:00|58.237|    58.355|   57.95333| 0.6883207|\n",
      "|2020-08-27 08:04:00| 58.14|    58.321|  57.933334| 0.6647098|\n",
      "|2020-08-27 08:06:00|58.146|    58.327|  57.843353| 0.8291988|\n",
      "|2020-08-27 08:07:00|58.155|    58.318|   57.80336| 0.8824694|\n",
      "|2020-08-27 08:08:00|58.143|    58.282|   57.80336|0.82124585|\n",
      "|2020-08-27 08:20:00|58.117|    58.307|   57.81136| 0.8500526|\n",
      "|2020-08-27 08:22:00| 58.15|    58.242|   57.86335| 0.6501338|\n",
      "+-------------------+------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Convert data back to Spark dataframes and cast the appropriate data types and print schema to verify^^^'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tree_spark = spark.createDataFrame(valid_tree)\n",
    "valid_tree_spark=valid_tree_spark.withColumn('time', col('time').cast('Timestamp'))\n",
    "valid_tree_spark=valid_tree_spark.withColumn('close', col('close').cast('Float'))\n",
    "valid_tree_spark=valid_tree_spark.withColumn('LTC_future', col('LTC_future').cast('Float'))\n",
    "valid_tree_spark=valid_tree_spark.withColumn('Predictions', col('Predictions').cast('Float'))\n",
    "valid_tree_spark=valid_tree_spark.withColumn('Error_%', col('Error_%').cast('Float'))\n",
    "valid_tree_spark.printSchema()\n",
    "valid_tree_spark.show()\n",
    "\"\"\"^^^Convert data back to Spark dataframes and cast the appropriate data types and print schema to verify^^^\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "valid_lr_spark = spark.createDataFrame(valid_lr)\n",
    "valid_lr_spark=valid_lr_spark.withColumn('time', col('time').cast('Timestamp'))\n",
    "valid_lr_spark=valid_lr_spark.withColumn('close', col('close').cast('Float'))\n",
    "valid_lr_spark=valid_lr_spark.withColumn('LTC_future', col('LTC_future').cast('Float'))\n",
    "valid_lr_spark=valid_lr_spark.withColumn('Predictions', col('Predictions').cast('Float'))\n",
    "valid_lr_spark=valid_lr_spark.withColumn('Error_%', col('Error_%').cast('Float'))\n",
    "valid_lr_spark.printSchema()\n",
    "valid_lr_spark.show()\n",
    "\"\"\"^^^Convert data back to Spark dataframes and cast the appropriate data types and print schema to verify^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_tree_spark dataframe shape: (30, 5)\n",
      "valid_lr_spark dataframe shape: (30, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'^^^Data Quality check #2-- Checking shape of dataframes^^^'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'valid_tree_spark dataframe shape:',(valid_tree_spark.count(), len(valid_tree_spark.columns)))\n",
    "#valid_tree_spark.select([count(when(isnan(c), c)).alias(c) for c in valid_tree_spark.columns]).show()\n",
    "\n",
    "print(f'valid_lr_spark dataframe shape:',(valid_lr_spark.count(), len(valid_lr_spark.columns)))\n",
    "#valid_lr_spark.select([count(when(isnan(c), c)).alias(c) for c in valid_lr_spark.columns]).show()\n",
    "\n",
    "\"\"\"^^^Data Quality check #2-- Checking shape of dataframes^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path s3a://dend-capstone-output/valid_tree_spark already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o630.parquet.\n: org.apache.spark.sql.AnalysisException: path s3a://dend-capstone-output/valid_tree_spark already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9494e554cb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_tree_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://dend-capstone-output/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'valid_tree_spark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_lr_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://dend-capstone-output/\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'valid_lr_spark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Write parquet files to S3 files & partition by time: COMPLETE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\"\"\"^^^Write decision tree and linear regression tables to S3 bucket as parquet files partitioned on 'time'^^^\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path s3a://dend-capstone-output/valid_tree_spark already exists.;'"
     ]
    }
   ],
   "source": [
    "valid_tree_spark.write.partitionBy('time').parquet(\"s3a://dend-capstone-output/\"+ 'valid_tree_spark')\n",
    "valid_lr_spark.write.partitionBy('time').parquet(\"s3a://dend-capstone-output/\"+ 'valid_lr_spark')\n",
    "print('Write parquet files to S3 files & partition by time: COMPLETE')\n",
    "\n",
    "\"\"\"^^^Write decision tree and linear regression tables to S3 bucket as parquet files partitioned on 'time'^^^\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL PROCESSING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    spark\n",
    "    input_data = \"s3a://dend-capstone-crypto-bucket/\"\n",
    "    output_data = \"s3a://dend-capstone-output/\"\n",
    "    process_ltc\n",
    "    #write_parq()\n",
    "    \n",
    "    #process_xrp(spark, input_data, output_data)\n",
    "    print('ETL PROCESSING COMPLETE!')\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
